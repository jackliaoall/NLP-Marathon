{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"pytorch_env","language":"python","name":"pytorch_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Day14-word2vec的高速化_作業.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"935eQbKYGJ40"},"source":["### 作業目的: 更加了解word2vec高速化\n","本次作業主要是幫同學更熟悉與了解透過各項技巧來加速word2vec的原理，同學可以參考章節講義來回答下列問題。"]},{"cell_type":"markdown","metadata":{"id":"6ZTHpRn_GJ47"},"source":["### Q1 - 請問word2vec原本的設計有何問題以及可以怎麼對相對應的問題做改善?\n","\n","\n","Answer:\n","\n","輸入的字詞利用one-hot編碼，會佔據過多的記憶體 (需要儲存 100000 個元素)，輸入與輸出層的矩陣過於龐大，因此在計算上會需要消耗較多的資源與時間，採用Embedding層可以順利解決原本word2vec中one-hot編碼與輸入輸出層矩陣運算的問題"]},{"cell_type":"markdown","metadata":{"id":"Nr4VLDXNGJ47"},"source":["### Q2 - 請問在Negative Sampling中的次方係數，會如何影響字詞的抽取?\n","Hint: 如何影響高頻詞與低頻詞的抽取機率\n","\n","Answer:\n","無"]}]}