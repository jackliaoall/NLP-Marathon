{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型搭建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearBNAC(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, bias=True, dropout=0.3, is_output=False):\n",
    "        super(LinearBNAC, self).__init__()\n",
    "        if is_output and out_channels==1:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Sigmoid()\n",
    "            )\n",
    "        elif is_output:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Softmax(dim=1)\n",
    "            )   \n",
    "        else:\n",
    "            self.linear = nn.Sequential(\n",
    "                nn.Linear(in_channels, out_channels, bias=bias),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.LeakyReLU(inplace=True)\n",
    "            )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        out=self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dimention, output_classes=1):\n",
    "        super(Model, self).__init__()\n",
    "        self.layer1 = LinearBNAC(input_dimention, 128)\n",
    "        self.layer2 = LinearBNAC(128, 64)\n",
    "        self.layer3 = LinearBNAC(64, 32)\n",
    "        self.output = LinearBNAC(32, output_classes, is_output=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.output(x)\n",
    "        return x \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=500,output_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer1.linear.0.weight torch.Size([128, 500])\n",
      "layer1.linear.0.bias torch.Size([128])\n",
      "layer1.linear.2.weight torch.Size([128])\n",
      "layer1.linear.2.bias torch.Size([128])\n",
      "layer2.linear.0.weight torch.Size([64, 128])\n",
      "layer2.linear.0.bias torch.Size([64])\n",
      "layer2.linear.2.weight torch.Size([64])\n",
      "layer2.linear.2.bias torch.Size([64])\n",
      "layer3.linear.0.weight torch.Size([32, 64])\n",
      "layer3.linear.0.bias torch.Size([32])\n",
      "layer3.linear.2.weight torch.Size([32])\n",
      "layer3.linear.2.bias torch.Size([32])\n",
      "output.linear.0.weight torch.Size([1, 32])\n",
      "output.linear.0.bias torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for name,_ in model.named_parameters():\n",
    "    print(name, _.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "input_features = 500\n",
    "dummy_input = torch.randn(batch_size, input_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6246],\n",
      "        [0.4512],\n",
      "        [0.5598],\n",
      "        [0.5134],\n",
      "        [0.4838],\n",
      "        [0.4538],\n",
      "        [0.4384],\n",
      "        [0.6206],\n",
      "        [0.4373],\n",
      "        [0.3914],\n",
      "        [0.3709],\n",
      "        [0.4022]], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = model(dummy_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.4442, grad_fn=<BinaryCrossEntropyBackward>) tensor(0.8147, grad_fn=<BinaryCrossEntropyBackward>)\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import BCELoss, BCEWithLogitsLoss\n",
    "\n",
    "sigmoid = nn.Sigmoid()\n",
    "\n",
    "prediction = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "\n",
    "## 將每一組 prediciotn, target 算出來的值相加\n",
    "criterios = BCELoss(reduction='sum')\n",
    "loss_1 = criterios(sigmoid(prediction), target)\n",
    "\n",
    "## 將每一組 prediciotn, target 算出來的值平均\n",
    "criterios = BCELoss(reduction='mean')\n",
    "loss_2 = criterios(sigmoid(prediction), target)\n",
    "\n",
    "print(loss_1, loss_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8147, grad_fn=<BinaryCrossEntropyWithLogitsBackward>)\n"
     ]
    }
   ],
   "source": [
    "## BCEWithLogitsLoss自帶 sigmoid 功能\n",
    "criterios = BCEWithLogitsLoss(reduction='mean')\n",
    "loss_3 = criterios(prediction, target)\n",
    "assert loss_2 == loss_3 ## 應該要與 output2相同\n",
    "print(loss_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import NLLLoss, LogSoftmax, CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CrossEntropyLoss = LogSoftmax + NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.randn(2, 3)\n",
    "ground_truth = torch.tensor([2, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* LogSoftmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeff.yang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:3: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/jeff.yang/opt/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "log_softmax = LogSoftmax()\n",
    "\n",
    "output = log_softmax(prediction)\n",
    "\n",
    "softmax = nn.Softmax()\n",
    "assert '{:.4f}'.format(output.sum()) == '{:.4f}'.format(torch.log(softmax(prediction)).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* NLLLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8571)\n"
     ]
    }
   ],
   "source": [
    "criterion = NLLLoss()\n",
    "loss_1 = criterion(output, ground_truth)\n",
    "print(loss_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 將NLLLoss拆解"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8571)\n"
     ]
    }
   ],
   "source": [
    "ground_truth_onehot = torch.FloatTensor(prediction.shape)\n",
    "ground_truth_onehot.zero_()\n",
    "ground_truth_onehot.scatter_(1, ground_truth.reshape(-1,1), 1)\n",
    "loss_count = - torch.mul(ground_truth_onehot, output).sum(-1).mean()\n",
    "assert '{:.4f}'.format(loss_1) == '{:.4f}'.format(loss_count)\n",
    "print(loss_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8571)\n"
     ]
    }
   ],
   "source": [
    "loss_2 = criterion(prediction, ground_truth)\n",
    "assert '{:.4f}'.format(loss_1) == '{:.4f}'.format(loss_2)\n",
    "print(loss_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MSE == L2 loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!['pic'](l1_l2_smooth.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss, L1Loss, SmoothL1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = torch.randn(3, 5, requires_grad=True)\n",
    "target = torch.randn(3, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* MSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1713, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = MSELoss()\n",
    "output = criterion(prediction, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* L1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7915, grad_fn=<L1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = L1Loss()\n",
    "output = criterion(prediction, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* SmoothL1Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4373, grad_fn=<SmoothL1LossBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = SmoothL1Loss()\n",
    "output = criterion(prediction, target)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(input_dimention=500,output_classes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* params : iterable of parameters\n",
    "* lr : learning rate\n",
    "* weight_decay : (L2) Regularization (正則化) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(params=model.parameters(), lr=1e-3, momentum=0.9, weight_decay=1e-3)\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-3, weight_decay=1e-3)\n",
    "optimizer = optim.RMSprop(params=model.parameters(), lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0,\\\n",
    "                          momentum=0, centered=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 利用 optimizer.step() 來實現參數更新"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 12\n",
    "input_features = 500\n",
    "dummy_input = torch.randn(batch_size, input_features)\n",
    "\n",
    "prediction = model(dummy_input)\n",
    "target = torch.empty(12, dtype=torch.float).random_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0309,  0.0286, -0.0084,  ..., -0.0153, -0.0038,  0.0111],\n",
      "        [-0.0070, -0.0294,  0.0051,  ...,  0.0125, -0.0431,  0.0296],\n",
      "        [ 0.0400, -0.0368,  0.0265,  ...,  0.0180,  0.0042,  0.0041],\n",
      "        ...,\n",
      "        [-0.0134, -0.0248,  0.0377,  ..., -0.0387, -0.0249, -0.0385],\n",
      "        [-0.0039, -0.0131, -0.0083,  ...,  0.0113, -0.0177,  0.0394],\n",
      "        [-0.0050,  0.0114,  0.0042,  ...,  0.0302, -0.0015, -0.0022]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : None\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterios = BCELoss(reduction='mean')\n",
    "loss = criterios(prediction.reshape(-1), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[-0.0309,  0.0286, -0.0084,  ..., -0.0153, -0.0038,  0.0111],\n",
      "        [-0.0070, -0.0294,  0.0051,  ...,  0.0125, -0.0431,  0.0296],\n",
      "        [ 0.0400, -0.0368,  0.0265,  ...,  0.0180,  0.0042,  0.0041],\n",
      "        ...,\n",
      "        [-0.0134, -0.0248,  0.0377,  ..., -0.0387, -0.0249, -0.0385],\n",
      "        [-0.0039, -0.0131, -0.0083,  ...,  0.0113, -0.0177,  0.0394],\n",
      "        [-0.0050,  0.0114,  0.0042,  ...,  0.0302, -0.0015, -0.0022]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-0.0136, -0.0266,  0.0739,  ...,  0.0317, -0.0294, -0.0088],\n",
      "        [ 0.0222,  0.0095, -0.0154,  ...,  0.0556, -0.0070,  0.0026],\n",
      "        [-0.0034, -0.0127, -0.0272,  ..., -0.0049, -0.0230, -0.0004],\n",
      "        ...,\n",
      "        [ 0.0166, -0.0325, -0.0516,  ...,  0.0019, -0.0058,  0.0084],\n",
      "        [ 0.0040,  0.0156,  0.0064,  ..., -0.0193,  0.0008,  0.0053],\n",
      "        [-0.0104, -0.0069, -0.0242,  ..., -0.0024, -0.0174, -0.0001]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[ 0.0691,  0.1286, -0.1084,  ..., -0.1153,  0.0962,  0.1111],\n",
      "        [-0.1070, -0.1294,  0.1051,  ..., -0.0875,  0.0569, -0.0704],\n",
      "        [ 0.1400,  0.0632,  0.1265,  ...,  0.1180,  0.1042,  0.1041],\n",
      "        ...,\n",
      "        [-0.1134,  0.0752,  0.1377,  ..., -0.1387,  0.0751, -0.1385],\n",
      "        [-0.1039, -0.1131, -0.1083,  ...,  0.1113, -0.1177, -0.0606],\n",
      "        [ 0.0950,  0.1114,  0.1042,  ...,  0.1302,  0.0985,  0.0977]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[-0.0136, -0.0266,  0.0739,  ...,  0.0317, -0.0294, -0.0088],\n",
      "        [ 0.0222,  0.0095, -0.0154,  ...,  0.0556, -0.0070,  0.0026],\n",
      "        [-0.0034, -0.0127, -0.0272,  ..., -0.0049, -0.0230, -0.0004],\n",
      "        ...,\n",
      "        [ 0.0166, -0.0325, -0.0516,  ...,  0.0019, -0.0058,  0.0084],\n",
      "        [ 0.0040,  0.0156,  0.0064,  ..., -0.0193,  0.0008,  0.0053],\n",
      "        [-0.0104, -0.0069, -0.0242,  ..., -0.0024, -0.0174, -0.0001]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight : Parameter containing:\n",
      "tensor([[ 0.0691,  0.1286, -0.1084,  ..., -0.1153,  0.0962,  0.1111],\n",
      "        [-0.1070, -0.1294,  0.1051,  ..., -0.0875,  0.0569, -0.0704],\n",
      "        [ 0.1400,  0.0632,  0.1265,  ...,  0.1180,  0.1042,  0.1041],\n",
      "        ...,\n",
      "        [-0.1134,  0.0752,  0.1377,  ..., -0.1387,  0.0751, -0.1385],\n",
      "        [-0.1039, -0.1131, -0.1083,  ...,  0.1113, -0.1177, -0.0606],\n",
      "        [ 0.0950,  0.1114,  0.1042,  ...,  0.1302,  0.0985,  0.0977]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "grad : tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "print('weight : {}'.format(model.layer1.linear[0].weight))\n",
    "print('\\n')\n",
    "print('grad : {}'.format(model.layer1.linear[0].weight.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
