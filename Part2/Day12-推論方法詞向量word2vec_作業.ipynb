{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Day12-推論方法詞向量word2vec_作業.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"4ON4t34iVkB5"},"source":["### 作業目的: 了解為何需要推論方法的詞向量及其優缺點\n","本次作業主要為思考題，請學員根據題目思考適合的回答"]},{"cell_type":"markdown","metadata":{"id":"d3QTJvF4VkB8"},"source":["### Question:\n","本次課程學習了word2vec中的兩個模型CBOW與Skip-gram，請學員思考與比較此兩種模型的優缺點?"]},{"cell_type":"markdown","metadata":{"id":"g4lyhjWbVkB9"},"source":["### Answer:\n","\n","* **CBOW**:\n","\n","在cbow方法中，是用周圍詞預測中心詞，從而利用中心詞的預測結果情況，使用GradientDesent方法，不斷的去調整周圍詞的向量。當訓練完成之後，每個詞都會作為中心詞，把周圍詞的詞向量進行了調整，這樣也就獲得了整個文本裏面所有詞的詞向量。\n","\n","cbow預測行為的次數跟整個文本的詞數幾乎是相等的（每次預測行為才會進行一次backpropgation, 而往往這也是最耗時的部分），復雜度大概是O(V);\n","\n","* **Skip-gram**:\n","\n","在skip-gram裏面，每個詞在作為中心詞的時候，實際上是 1個學生 VS K個老師，K個老師（周圍詞）都會對學生（中心詞）進行“專業”的訓練，這樣學生（中心詞）的“能力”（向量結果）相對就會紮實（準確）一些，但是這樣肯定會使用更長的時間；\n","\n","cbow是 1個老師 VS K個學生，K個學生（周圍詞）都會從老師（中心詞）那裏學習知識，但是老師（中心詞）是一視同仁的，教給大家的一樣的知識。至於你學到了多少，還要看下一輪（假如還在窗口內），或者以後的某一輪，你還有機會加入老師的課堂當中（再次出現作為周圍詞），跟著大家一起學習，然後進步一點。因此相對skip-gram，你的業務能力肯定沒有人家強，但是對於整個訓練營（訓練過程）來說，這樣肯定效率高，速度更快。\n"]}]}